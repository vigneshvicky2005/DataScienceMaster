{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2fda5d-b1bd-429e-b47c-126b22759d8c",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a648557-301e-4a57-838f-7906a93c29b4",
   "metadata": {},
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d965d-03f1-4aec-a644-67d0a22b54ea",
   "metadata": {},
   "source": [
    "### Support Vector Machines-1 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceac74f-1567-46f9-aa7a-fe803fe3a867",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Polynomial functions and kernel functions have a close relationship in machine learning algorithms, particularly in the context of support vector machines (SVMs).\n",
    "\n",
    "In SVMs, the goal is to find a decision boundary that separates data points belonging to different classes. Kernel functions play a crucial role in SVMs by transforming the input data into a higher-dimensional feature space, where it may become linearly separable.\n",
    "\n",
    "Polynomial functions can be used as kernel functions in SVMs. When a polynomial kernel function is applied to the input data, it implicitly computes the dot product between the transformed feature vectors in the higher-dimensional space. The polynomial kernel function is defined as:\n",
    "\n",
    "K(x, y) = (γ * ⟨x, y⟩ + c)^d\n",
    "\n",
    "Here, x and y are the input vectors, ⟨x, y⟩ denotes the dot product between x and y, γ is a scaling factor, c is a constant term, and d is the degree of the polynomial.\n",
    "\n",
    "By using the polynomial kernel function, the SVM can effectively capture nonlinear relationships between the input variables without explicitly computing the transformations to the higher-dimensional space. The degree parameter (d) determines the complexity of the polynomial transformation.\n",
    "\n",
    "In summary, polynomial functions can be used as kernel functions in SVMs to implicitly perform nonlinear transformations of the input data, allowing for the separation of data points in higher-dimensional feature spaces.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q2\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, you can follow these steps:\n",
    "\n",
    "Step 1: Install Scikit-learn\n",
    "If you haven't installed Scikit-learn, you can do so by running the following command:\n",
    "\n",
    "Copy code\n",
    "pip install scikit-learn\n",
    "Step 2: Import the necessary modules\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "Step 3: Generate sample data\n",
    "You can create a sample dataset using the make_classification function in Scikit-learn. Here's an example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=1)\n",
    "Step 4: Split the data into training and testing sets\n",
    "\n",
    "python\n",
    "Copy code\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "Step 5: Create and train the SVM model\n",
    "\n",
    "python\n",
    "Copy code\n",
    "model = svm.SVC(kernel='poly', degree=3)\n",
    "model.fit(X_train, y_train)\n",
    "In this example, we are using SVC (Support Vector Classification) from Scikit-learn's svm module. The kernel parameter is set to 'poly' to specify the polynomial kernel, and the degree parameter determines the degree of the polynomial.\n",
    "\n",
    "Step 6: Make predictions\n",
    "\n",
    "python\n",
    "Copy code\n",
    "y_pred = model.predict(X_test)\n",
    "Step 7: Evaluate the model\n",
    "You can use various evaluation metrics to assess the performance of your model. For example:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "This is a basic outline of how to implement an SVM with a polynomial kernel in Python using Scikit-learn. You can adjust the parameters and experiment with different values to find the best model for your specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q3\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "In Support Vector Regression (SVR), the value of epsilon (ε) represents the width of the epsilon-insensitive tube. The epsilon-insensitive tube is the range within which errors are considered acceptable and do not contribute to the loss function during training.\n",
    "\n",
    "Increasing the value of epsilon typically leads to an increase in the number of support vectors in SVR. Here's why:\n",
    "\n",
    "Larger Epsilon Tolerance: When the value of epsilon is increased, it allows more training samples to fall within the epsilon-insensitive tube without contributing to the loss function. This means that more data points are considered acceptable, even if they deviate from the predicted output by a larger margin. Consequently, the SVR model becomes less strict in fitting the training data precisely.\n",
    "\n",
    "Expansion of the Margin: As epsilon increases, the margin around the regression line also expands. The margin is the region between the support vectors and the regression line within which training samples can reside. With a wider margin, more data points can be encompassed within it, leading to an increase in the number of support vectors. The support vectors are the training samples that lie on or within the margin and play a critical role in defining the regression line.\n",
    "\n",
    "It's important to note that the exact impact of epsilon on the number of support vectors may vary depending on the specific dataset and problem. In some cases, increasing epsilon may not have a significant effect on the number of support vectors if the data points are already well within the expanded margin.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q4\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "The performance of Support Vector Regression (SVR) is influenced by several parameters: the choice of kernel function, the C parameter, the epsilon parameter, and the gamma parameter. Here's an explanation of each parameter and how it affects SVR performance:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "SVR uses a kernel function to transform the input data into a higher-dimensional feature space. The choice of kernel function determines the type of transformations performed.\n",
    "Examples of kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "If the data has a nonlinear relationship, using a nonlinear kernel such as polynomial, RBF, or sigmoid can improve performance.\n",
    "For linearly separable data, a linear kernel can be sufficient.\n",
    "C Parameter:\n",
    "\n",
    "The C parameter controls the trade-off between the model's complexity and the degree of error allowed in the training data.\n",
    "A smaller C value allows for a larger margin and more errors in the training data, leading to a simpler model with potentially more generalized performance.\n",
    "A larger C value enforces a smaller margin, aiming for fewer errors in the training data and potentially a more complex model that closely fits the training data.\n",
    "If overfitting is observed, decreasing C might be beneficial, while increasing C can be useful to reduce underfitting.\n",
    "Epsilon Parameter:\n",
    "\n",
    "The epsilon parameter (ε) determines the width of the epsilon-insensitive tube. It specifies the threshold within which errors are considered acceptable and do not contribute to the loss function.\n",
    "A larger epsilon value allows for a wider tube, accepting larger errors, potentially leading to a more tolerant and generalized model.\n",
    "A smaller epsilon value makes the model more sensitive to errors and aims for a precise fit to the training data.\n",
    "Increasing epsilon may be helpful when the data has more noise or when a less precise fit is desired.\n",
    "Gamma Parameter:\n",
    "\n",
    "The gamma parameter (γ) defines the influence of each training sample in the SVR model.\n",
    "A smaller gamma value implies a wider influence range, resulting in smoother decision boundaries. This can be useful when the data has less variability or noise.\n",
    "A larger gamma value gives more weight to nearby training samples, potentially leading to more complex and intricate decision boundaries that fit the training data more closely.\n",
    "Decreasing gamma might be beneficial when overfitting is observed, while increasing gamma can be useful when underfitting is an issue.\n",
    "It's important to note that the impact of these parameters can vary depending on the dataset and problem at hand. It is generally recommended to perform hyperparameter tuning, such as using grid search or cross-validation, to find the optimal values for these parameters for a specific problem.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q5\n",
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use.\n",
    "ChatGPT\n",
    "Certainly! Here's an example code implementation for the steps you mentioned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd8018-063e-431b-8169-a0e50937d8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('your_dataset.csv')  # Replace 'your_dataset.csv' with your actual dataset file\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data.drop('target_column', axis=1)  # Replace 'target_column' with the column containing the target variable\n",
    "y = data['target_column']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Preprocess the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of SVC classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels of the testing data\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "svm_tuned = SVC(**best_params)\n",
    "svm_tuned.fit(X_scaled, y)\n",
    "\n",
    "# Save the trained classifier to a file\n",
    "joblib.dump(svm_tuned, 'svm_tuned_model.pkl')  # Replace 'svm_tuned_model.pkl' with your desired file name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
