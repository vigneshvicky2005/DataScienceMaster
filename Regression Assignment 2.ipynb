{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360b16e3-b7a2-4381-8f95-1c465b8ecf0c",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e973d7-7a30-489a-a5ab-b5aaefe63603",
   "metadata": {},
   "source": [
    "## Data science Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc42bc-68cf-425e-94d4-126d3b05b603",
   "metadata": {},
   "source": [
    "### Regression Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd64bc1-aaf5-4cf6-9a9a-2d3c34763f28",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "How To Interpret R-squared in Regression Analysis\n",
    "By Jim Frost 126 Comments\n",
    "\n",
    "R-squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of the variance in the dependent variable that the independent variables explain collectively. R-squared measures the strength of the relationship between your model and the dependent variable on a convenient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ab80f-29b0-480b-857e-7ab9a28c4883",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "R-Squared vs. Adjusted R-Squared: An Overview\n",
    "R-squared and adjusted R-squared enable investors to measure the performance of a mutual fund against that of a benchmark. Investors may also use them to calculate the performance of their portfolio against a given benchmark.\n",
    "\n",
    "In the world of investing, R-squared is expressed as a percentage between 0 and 100, with 100 signaling perfect correlation and zero no correlation at all. The figure does not indicate how well a particular group of securities is performing. It only measures how closely the returns align with those of the measured benchmark. It is also backwards-looking—it is not a predictor of future results.\n",
    "\n",
    "Adjusted R-squared can provide a more precise view of that correlation by also taking into account how many independent variables are added to a particular model against which the stock index is measured. This is done because such additions of independent variables usually increase the reliability of that model—meaning, for investors, the correlation with the index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377f20d-1278-4a6a-bb7d-677c15469463",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "When we fit linear regression models we often calculate the R-squared value of the model.\n",
    "\n",
    "The R-squared value is the proportion of the variance in the response variable that can be explained by the predictor variables in the model.\n",
    "\n",
    "The value for R-squared can range from 0 to 1 where:\n",
    "\n",
    "A value of 0 indicates that the response variable cannot be explained by the predictor variables at all.\n",
    "A value of 1 indicates that the response variable can be perfectly explained by the predictor variables.\n",
    "Although this metric is commonly used to assess how well a regression model fits a dataset, it has one serious drawback:\n",
    "\n",
    "The drawback of R-squared:\n",
    "\n",
    " \n",
    "\n",
    "R-squared will always increase when a new predictor variable is added to the regression model.\n",
    "\n",
    "Even if a new predictor variable is almost completely unrelated to the response variable, the R-squared value of the model will increase, if only by a small amount.\n",
    "\n",
    "For this reason, it’s possible that a regression model with a large number of predictor variables has a high R-squared value, even if the model doesn’t fit the data well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b5277a-1610-47f6-88af-433149d46475",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "Let’s understand this with an example. Say, I want to predict the salary of a data scientist based on the number of years of experience. So, salary is my target variable (Y) and experience is the independent variable(X). I have some random data on X and Y and we will use Linear Regression to predict salary. Let’s use pandas and scikit-learn for data loading and creating linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac8c950-bebf-427a-85a1-58c0dea4a09a",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "Regression models are used to quantify the relationship between one or more predictor variables and a response variable.\n",
    "\n",
    "Whenever we fit a regression model, we want to understand how well the model is able to use the values of the predictor variables to predict the value of the response variable.\n",
    "\n",
    "Two metrics we often use to quantify how well a model fits a dataset are the mean absolute error (MAE) and the root mean squared error (RMSE), which are calculated as follows:\n",
    "\n",
    "MAE: A metric that tells us the mean absolute difference between the predicted values and the actual values in a dataset. The lower the MAE, the better a model fits a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b718f-ac6e-448e-8247-72c2b781d45c",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "After understanding the basic principles of linear regression and gradient descent, It is time to move forward a bit and review some techniques that improve the performance of ordinary linear regression models. The most common techniques are LASSO regularization (L1 Regularization) and Ridge Regularization (L2 Regularization). First, we need to know what “Regularization” means. Simply Regularization is the process of adding information to prevent over-fitting.\n",
    "\n",
    "The over-fitting problem occurs when the error of the model is minimum in the training phase, but the performance of the model with testing data points is poor. That means that the model is not generalized and cannot be used in production.\n",
    "\n",
    "LASSO Regularization:\n",
    "LASSO stands for Least Absolute Shrinkable and Selection Operator. As mentioned in the regularization definition, it is the process of adding information to prevent the over-fitting problem, so a small modification will be done to the cost function of the ordinary least square as shown below.\n",
    "\n",
    "\n",
    "Cost Function of LASSO Regression Model\n",
    "To understand the effect of this additional term “Penalty term”, let’s assume that the best-fit line is passing through all data points hence the sum of square error is zero. The additional term ( λ |slope| )should be minimized to minimize the cost function. Minimizing the slope means that it makes the line less steep and this will not make the line passing through all data points and this will help to prevent over-fitting.\n",
    "The next question we should ask is “what is λ?”. It is the regularization parameter which is a positive integer. If λ is too high, this will minimize or “shrink” the slope to zero. This is the main added value of LASSO. It can suppress the coefficients of useless features (highly correlated features) and hence, it makes the feature selection for our linear regression model. On the other hand, If λ is equal to zero, the loss function will turn into the “Ordinary Least Square” model. The regularization parameter (λ) can be determined by cross-validation in a way that avoids under-fitting (when λ is too high) and over-fitting (when λ is too low).\n",
    "\n",
    "\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "Let’s say we want to predict if a student will land a job interview based on her resume.\n",
    "\n",
    "Now, assume we train a model from a dataset of 10,000 resumes and their outcomes.\n",
    "\n",
    "Next, we try the model out on the original dataset, and it predicts outcomes with 99% accuracy… wow!\n",
    "\n",
    "But now comes the bad news.\n",
    "\n",
    "When we run the model on a new (“unseen”) dataset of resumes, we only get 50% accuracy… uh-oh!\n",
    "\n",
    "Our model doesn’t generalize well from our training data to unseen data.\n",
    "\n",
    "This is known as overfitting, and it’s a common problem in machine learning and data science.\n",
    "\n",
    "In fact, overfitting occurs in the real world all the time. You only need to turn on the news channel to hear examples:\n",
    "\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "Ridge Regression\n",
    "Ridge Regression also called Tikhonov regularization\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression: a regularization term (equation 1) is added to the cost function. This forces the learning algorithm to fit the data and keep the model weights as small as possible.\n",
    "\n",
    "\n",
    "Equation 1\n",
    "Note that while evaluating the model we do not use the regularized term in the cost function.\n",
    "\n",
    "The hyperparameter α controls how much you want to regularize the model. If α = 0 then Ridge Regression is just Linear Regression. If α is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.\n",
    "\n",
    "Let’s look at the loss function of ridge regression.\n",
    "\n",
    "\n",
    "Loss function\n",
    "So, quite an easy equation for training a model precisely we can say correcting the model.\n",
    "\n",
    "Moving forward let’s see how can we implement it using sklearn. Yes, it’s no big deal implementing models using sklearn, you just need 3 lines to prepare the model for prediction, but the important thing is hyperparameter tuning which is a skill that a data scientist or a machine learning engineer needs to develop.\n",
    "\n",
    "Let’s look at the implementation. Note that our goal here is understanding models and not hyperparameter tunning.\n",
    "\n",
    "Firstly, we will generate some dummy data using numpy.\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "Ridge Regression\n",
    "Ridge Regression also called Tikhonov regularization\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression: a regularization term (equation 1) is added to the cost function. This forces the learning algorithm to fit the data and keep the model weights as small as possible.\n",
    "\n",
    "\n",
    "Equation 1\n",
    "Note that while evaluating the model we do not use the regularized term in the cost function.\n",
    "\n",
    "The hyperparameter α controls how much you want to regularize the model. If α = 0 then Ridge Regression is just Linear Regression. If α is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.\n",
    "\n",
    "Let’s look at the loss function of ridge regression.\n",
    "\n",
    "\n",
    "Loss function\n",
    "So, quite an easy equation for training a model precisely we can say correcting the model.\n",
    "\n",
    "Moving forward let’s see how can we implement it using sklearn. Yes, it’s no big deal implementing models using sklearn, you just need 3 lines to prepare the model for prediction, but the important thing is hyperparameter tuning which is a skill that a data scientist or a machine learning engineer needs to develop.\n",
    "\n",
    "Let’s look at the implementation. Note that our goal here is understanding models and not hyperparameter tunning.\n",
    "\n",
    "Firstly, we will generate some dummy data using numpy.\n",
    "\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "Ridge Regression\n",
    "Ridge Regression also called Tikhonov regularization\n",
    "\n",
    "Ridge Regression is a regularized version of Linear Regression: a regularization term (equation 1) is added to the cost function. This forces the learning algorithm to fit the data and keep the model weights as small as possible.\n",
    "\n",
    "\n",
    "Equation 1\n",
    "Note that while evaluating the model we do not use the regularized term in the cost function.\n",
    "\n",
    "The hyperparameter α controls how much you want to regularize the model. If α = 0 then Ridge Regression is just Linear Regression. If α is very large, then all weights end up very close to zero and the result is a flat line going through the data’s mean.\n",
    "\n",
    "Let’s look at the loss function of ridge regression.\n",
    "\n",
    "\n",
    "Loss function\n",
    "So, quite an easy equation for training a model precisely we can say correcting the model.\n",
    "\n",
    "Moving forward let’s see how can we implement it using sklearn. Yes, it’s no big deal implementing models using sklearn, you just need 3 lines to prepare the model for prediction, but the important thing is hyperparameter tuning which is a skill that a data scientist or a machine learning engineer needs to develop.\n",
    "\n",
    "Let’s look at the implementation. Note that our goal here is understanding models and not hyperparameter tunning.\n",
    "\n",
    "Firstly, we will generate some dummy data using numpy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
