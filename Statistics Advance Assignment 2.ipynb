{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8389fff0-55e4-4684-a0c3-d6c544ff163d",
   "metadata": {},
   "source": [
    "## Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcd253-a53d-4a00-840a-0cdf464be3fc",
   "metadata": {},
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74156877-fe98-4a9c-bdcf-c633fc20800c",
   "metadata": {},
   "source": [
    "### Statistics Advance Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1f5c4-b8f1-4a5c-b7b3-d9220e158070",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n",
    "an example.\n",
    "\n",
    "    The Probability Mass Function (PMF) and Probability Density Function (PDF) are both mathematical functions used to describe the probability distribution of a random variable.\n",
    "\n",
    "    Probability Mass Function (PMF):\n",
    "\n",
    "    The PMF is used for discrete random variables, which take on a countable set of values. It gives the probability of each possible outcome. The PMF is defined as P(X = x), where X is the random variable and x represents a specific value it can take.\n",
    "\n",
    "  Example:\n",
    "\n",
    "   Consider a fair six-sided die. The random variable X represents the outcome of rolling the die. The PMF of X can be defined as follows:\n",
    "\n",
    "P(X = 1) = 1/6\n",
    "\n",
    "P(X = 2) = 1/6\n",
    "\n",
    "P(X = 3) = 1/6\n",
    "\n",
    "P(X = 4) = 1/6\n",
    "\n",
    "P(X = 5) = 1/6\n",
    "\n",
    "P(X = 6) = 1/6\n",
    "\n",
    "    In this example, the PMF gives the probability of each possible outcome (1, 2, 3, 4, 5, or 6) of rolling the die.\n",
    "\n",
    "    Probability Density Function (PDF):\n",
    "\n",
    "    The PDF is used for continuous random variables, which can take on any value within a certain range. The PDF represents the probability density at each point, but unlike the PMF, it does not give the probability of obtaining a specific value. Instead, the probability of an event occurring within a range of values is calculated by integrating the PDF over that range.\n",
    "\n",
    "    Example:\n",
    "  \n",
    "    Consider a continuous random variable X representing the height of adults. The PDF of X might follow a normal distribution, which is described by the bell-shaped curve. The PDF gives the relative likelihood of observing different heights.\n",
    "\n",
    "    The PDF of X would look like a smooth curve, and we could calculate the probability of an adult's height falling within a certain range, such as between 160 cm and 180 cm, by integrating the PDF over that range.\n",
    "\n",
    "     It's important to note that the area under the PDF curve represents the probability of the random variable falling within the entire range of possible values.\n",
    "\n",
    "     In summary, the PMF is used for discrete random variables and provides the probability of specific outcomes, while the PDF is used for continuous random variables and gives the relative likelihood of different values occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe953633-eac0-4e47-9d0d-2ed7b2bcedff",
   "metadata": {},
   "source": [
    "## Q2\n",
    "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n",
    "\n",
    "    The Cumulative Distribution Function (CDF) is a function that gives the cumulative probability distribution of a random variable. It provides the probability that the random variable takes on a value less than or equal to a given value.\n",
    "\n",
    "    The CDF is defined as F(x) = P(X ≤ x), where X is the random variable and x is a specific value.\n",
    "\n",
    "    Example:\n",
    "\n",
    "      Let's consider a continuous random variable X representing the weight of apples in grams. Suppose the CDF of X is given by:\n",
    "\n",
    "F(x) = 0.2x, for 0 ≤ x ≤ 5\n",
    "\n",
    "F(x) = 1, for x > 5\n",
    "\n",
    "     In this example, the CDF states that if we choose a random apple, the probability that its weight is less than or equal to 3 grams is F(3) = 0.2 * 3 = 0.6. Similarly, the probability that the weight is less than or equal to 5 grams is F(5) = 0.2 * 5 = 1.0.\n",
    "\n",
    "    Why CDF is used:\n",
    "\n",
    "     The CDF is used for several purposes:\n",
    "\n",
    "     Probability Calculation: The CDF allows us to calculate the probability that a random variable takes on a value within a specific range. For example, if we want to find the probability of X falling between two values a and b, we can use the CDF: P(a ≤ X ≤ b) = F(b) - F(a).\n",
    "\n",
    "     Determining Percentiles: The CDF can be used to find percentiles or quantiles of a distribution. For example, if we want to find the value x such that a certain percentage of the distribution lies below it, we can use the CDF. For instance, to find the 75th percentile, we look for the value x such that F(x) = 0.75.\n",
    "\n",
    "     Characterizing the Distribution: The shape and behavior of the CDF can provide insights into the characteristics of the random variable's distribution, such as the skewness, symmetry, or tails.\n",
    "\n",
    "      Comparison of Distributions: The CDF can be used to compare and analyze different distributions. By comparing the CDFs of multiple random variables, we can assess their similarities and differences.\n",
    "\n",
    "      In summary, the CDF provides the cumulative probability distribution of a random variable, allowing us to calculate probabilities, find percentiles, analyze distribution characteristics, and compare different distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb6790-6795-45d0-9b9d-fff9712294a4",
   "metadata": {},
   "source": [
    "## Q3\n",
    "Q3: What are some examples of situations where the normal distribution might be used as a model?\n",
    "Explain how the parameters of the normal distribution relate to the shape of the distribution.\n",
    "\n",
    "    The normal distribution, also known as the Gaussian distribution or bell curve, is a widely used probability distribution that appears in many real-world situations. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "    Heights of a Population: The heights of a large population tend to follow a normal distribution. The mean and standard deviation of the distribution can provide information about the average height and the spread of heights in the population.\n",
    "\n",
    "     Test Scores: Test scores, such as IQ tests or standardized exams, often approximate a normal distribution. The mean and standard deviation can be used to describe the average performance and the variability of scores.\n",
    "\n",
    "     Measurement Errors: Errors in measurements or observations often follow a normal distribution. For example, errors in laboratory measurements or errors in estimating distances or weights tend to be normally distributed.\n",
    "\n",
    "     Financial Returns: Daily or monthly financial returns of stocks or other financial instruments are often assumed to be normally distributed. The mean and standard deviation of the returns can help in modeling and predicting financial outcomes.\n",
    "\n",
    "    Biological Phenomena: Many biological measurements, such as blood pressure, enzyme activity, or gene expression levels, can be well-modeled by a normal distribution. The parameters of the distribution provide insights into the average level and variability of the biological phenomenon.\n",
    "    \n",
    "Now, let's discuss how the parameters of the normal distribution relate to the shape of the distribution:\n",
    "\n",
    "    Mean (μ): The mean determines the center or the peak of the distribution. It represents the average value around which the data is symmetrically distributed. Shifting the mean to the left or right moves the entire distribution along the horizontal axis.\n",
    "\n",
    "    Standard Deviation (σ): The standard deviation controls the spread or dispersion of the data. A smaller standard deviation results in a narrower distribution, while a larger standard deviation leads to a wider distribution. The standard deviation influences the shape of the bell curve and determines how much the data deviates from the mean.\n",
    "\n",
    "     By manipulating the mean and standard deviation, we can adjust the position and shape of the normal distribution to fit specific datasets or model different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960954cd-59df-4483-82b4-90b2889bcb0e",
   "metadata": {},
   "source": [
    "## Q4\n",
    "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n",
    "Distribution.\n",
    "\n",
    "    The normal distribution is of significant importance in various fields due to its many desirable properties and its prevalence in real-life phenomena. Here are some reasons why the normal distribution is important:\n",
    "\n",
    "    Central Limit Theorem: The normal distribution is a fundamental concept in statistics because of the Central Limit Theorem. According to this theorem, the sum or average of a large number of independent and identically distributed random variables tends to follow a normal distribution, regardless of the underlying distribution of the variables. This property allows us to make statistical inferences and estimate parameters in a wide range of applications.\n",
    "\n",
    "    Statistical Inference: Many statistical techniques, such as hypothesis testing, confidence intervals, and regression analysis, are based on assumptions of normality. When the data closely follows a normal distribution, these techniques are reliable and provide accurate results.\n",
    "\n",
    "    Data Analysis and Modeling: The normal distribution serves as a useful approximation for many real-life phenomena. By assuming a normal distribution, researchers and analysts can simplify data analysis and create models that accurately describe and predict outcomes in various fields.\n",
    "\n",
    "Now, let's look at a few real-life examples where the normal distribution is commonly observed:\n",
    "\n",
    "    IQ Scores: Intelligence quotient (IQ) scores are often assumed to follow a normal distribution. This allows for comparing individual scores to the population distribution and assessing intellectual abilities.\n",
    "\n",
    "    Heights and Weights: Human heights and weights, especially in large populations, tend to approximate a normal distribution. This distribution helps in understanding the typical height and weight ranges and comparing individuals to the population average.\n",
    "\n",
    "    Exam Scores: When a large number of students take an exam, their scores often exhibit a normal distribution. This helps in analyzing the performance of students, setting grading criteria, and identifying outliers.\n",
    "\n",
    "    Measurement Errors: In various scientific experiments and measurements, errors can follow a normal distribution. This assumption is essential for accurate estimation of the true values and assessing the precision of the measurements.\n",
    "\n",
    "    Financial Markets: In finance, stock returns, and other financial variables are often modeled using a normal distribution or variants like the log-normal distribution. This assumption helps in risk assessment, option pricing, and portfolio management.\n",
    "\n",
    "    Quality Control: When monitoring the quality of manufactured products, measurements such as length, weight, or volume often approximate a normal distribution. This allows for setting quality standards, identifying defective products, and making process improvements.\n",
    "\n",
    "    These examples highlight the ubiquity and practical significance of the normal distribution in various domains, making it a valuable tool for data analysis, modeling, and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e01501-b754-4196-b818-8f9d02edd8ac",
   "metadata": {},
   "source": [
    "## Q5\n",
    "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n",
    "Distribution and Binomial Distribution?\n",
    "\n",
    "    The Bernoulli distribution is a discrete probability distribution that models a random experiment with two possible outcomes: success (usually denoted as 1) or failure (usually denoted as 0). It is named after the Swiss mathematician Jacob Bernoulli. The distribution is characterized by a single parameter, p, which represents the probability of success.\n",
    "\n",
    "    The probability mass function (PMF) of the Bernoulli distribution is given by:\n",
    "\n",
    "    P(X = k) = p^k * (1 - p)^(1 - k)\n",
    "\n",
    "    where X is the random variable representing the outcome (0 or 1), and k can only take values 0 or 1.\n",
    "\n",
    "   Example:\n",
    "    \n",
    "    Let's consider a coin flip experiment. Suppose we have a fair coin, and we define success as getting a \"heads\" (H) and failure as getting a \"tails\" (T). The Bernoulli distribution can be used to model this experiment.\n",
    "\n",
    "     In this case, the parameter p (probability of success) is 0.5 because the coin is fair. Therefore, the PMF of the Bernoulli distribution for this coin flip experiment would be:\n",
    "\n",
    "P(X = 1) = 0.5^1 * (1 - 0.5)^(1 - 1) = 0.5\n",
    "\n",
    "P(X = 0) = 0.5^0 * (1 - 0.5)^(1 - 0) = 0.5\n",
    "\n",
    "    The Bernoulli distribution is the simplest form of a discrete distribution with two possible outcomes and is often used as a building block for more complex distributions.\n",
    "\n",
    "Now, let's discuss the difference between the Bernoulli distribution and the Binomial distribution:\n",
    "\n",
    "    Number of Trials:\n",
    "     \n",
    "     The Bernoulli distribution models a single trial or experiment with two possible outcomes (success or failure). On the other hand, the Binomial distribution models the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "    Number of Parameters:\n",
    "      \n",
    "      The Bernoulli distribution has a single parameter, p, representing the probability of success in a single trial. In contrast, the Binomial distribution has two parameters: n, the number of trials, and p, the probability of success in each trial.\n",
    "\n",
    "    Probability Mass Function (PMF):\n",
    "       \n",
    "       The PMF of the Bernoulli distribution gives the probabilities of the two possible outcomes (0 and 1). The PMF of the Binomial distribution gives the probabilities of different numbers of successes in n trials.\n",
    "\n",
    "     Distribution Shape:\n",
    "\n",
    "        The Bernoulli distribution is a discrete distribution with only two possible outcomes, resulting in a distribution with a single peak. The Binomial distribution is also a discrete distribution but can have multiple peaks depending on the values of n and p.\n",
    "\n",
    "    In summary, the Bernoulli distribution models a single trial with two outcomes, while the Binomial distribution models the number of successes in a fixed number of independent Bernoulli trials. The Bernoulli distribution has one parameter (p), while the Binomial distribution has two parameters (n and p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06b50-7404-4bbb-a650-38a8012a3e9e",
   "metadata": {},
   "source": [
    "## Q6\n",
    "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n",
    "is normally distributed, what is the probability that a randomly selected observation will be greater\n",
    "than 60? Use the appropriate formula and show your calculations.\n",
    "\n",
    "    To calculate the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 is greater than 60, we need to use the standard normal distribution and z-scores.\n",
    "\n",
    "    First, we calculate the z-score for the value 60 using the formula:\n",
    "\n",
    "      z = (x - μ) / σ\n",
    "\n",
    "    where x is the value (60), μ is the mean (50), and σ is the standard deviation (10).\n",
    "\n",
    "      z = (60 - 50) / 10 = 1\n",
    "\n",
    "    The z-score of 1 tells us that the value of 60 is one standard deviation above the mean in a standard normal distribution.\n",
    "\n",
    "    Next, we need to find the probability corresponding to this z-score using a standard normal distribution table or calculator. The probability we're interested in is the area under the curve to the right of the z-score.\n",
    "\n",
    "    Using a standard normal distribution table or calculator, we find that the probability corresponding to a z-score of 1 is approximately 0.8413.\n",
    "\n",
    "    Therefore, the probability that a randomly selected observation from the given dataset is greater than 60 is approximately 0.8413 or 84.13%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb2f39-9138-4061-9d15-14fabfe58d75",
   "metadata": {},
   "source": [
    "## Q7\n",
    "Q7: Explain uniform Distribution with an example.\n",
    "\n",
    "     The uniform distribution is a continuous probability distribution where all values within a specified interval are equally likely to occur. It is characterized by a constant probability density function (PDF) over the range of possible values.\n",
    "\n",
    "    The PDF of the uniform distribution is given by:\n",
    "\n",
    "      f(x) = 1 / (b - a), for a ≤ x ≤ b\n",
    "\n",
    "    where a and b are the lower and upper bounds of the distribution, respectively.\n",
    "\n",
    "    Example:\n",
    "\n",
    "     Let's consider an example of a uniform distribution representing the roll of a fair six-sided die. In this case, the random variable X represents the outcome of rolling the die. The uniform distribution is used because each outcome (1, 2, 3, 4, 5, 6) is equally likely.\n",
    "\n",
    "    The PDF of the uniform distribution for this example is:\n",
    "\n",
    "     f(x) = 1 / 6, for 1 ≤ x ≤ 6\n",
    "\n",
    "    In this case, the probability of rolling any specific value is 1/6 since there are six equally likely outcomes. The uniform distribution ensures that each outcome has an equal chance of occurring.\n",
    "\n",
    "    The uniform distribution is often used in scenarios where there is no inherent bias or preference towards any particular value within a given interval. It is used in areas such as random number generation, simulations, and statistical sampling.\n",
    " \n",
    "     It's important to note that there are discrete versions of the uniform distribution as well, where the possible values are restricted to integers within an interval. However, in this explanation, we focused on the continuous uniform distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dcea8-e7a5-4ba7-bb26-9b36bb871d2b",
   "metadata": {},
   "source": [
    "## Q8\n",
    "Q8: What is the z score? State the importance of the z score.\n",
    "\n",
    "    The z-score, also known as the standard score or standardized value, is a measure that quantifies how many standard deviations a given value is from the mean of a distribution. It allows for the comparison of data points from different distributions and provides insights into their relative positions.\n",
    "\n",
    "The formula to calculate the z-score for a value x, given the mean (μ) and standard deviation (σ) of the distribution, is:\n",
    "\n",
    "z = (x - μ) / σ\n",
    "\n",
    "The z-score indicates how many standard deviations a data point is above or below the mean. A positive z-score indicates a value above the mean, while a negative z-score indicates a value below the mean.\n",
    "\n",
    "Importance of the z-score:\n",
    "\n",
    "Standardization and Comparison: \n",
    "\n",
    "The z-score standardizes data by transforming it into a common scale, allowing for easy comparison and analysis of values from different distributions. It enables researchers, statisticians, and analysts to assess how an individual data point or observation compares to the overall distribution.\n",
    "\n",
    "Normal Distribution Analysis:\n",
    "\n",
    "The z-score is particularly useful when working with normally distributed data. In a standard normal distribution (mean = 0, standard deviation = 1), the z-score directly represents the proportion of data points below or above a given value. This allows for precise calculations of percentiles and probabilities.\n",
    "\n",
    "Outlier Detection: \n",
    "\n",
    "The z-score can help identify outliers in a dataset. Values that fall beyond a certain z-score threshold (typically ±2 or ±3) are considered unusual or extreme. This aids in detecting data points that deviate significantly from the expected pattern, potentially indicating errors, anomalies, or interesting phenomena.\n",
    "\n",
    "Hypothesis Testing: \n",
    "\n",
    "In hypothesis testing, the z-score plays a crucial role in determining the statistical significance of a sample mean difference or proportion difference. By comparing the observed z-score to the critical z-value associated with a specific confidence level, researchers can make conclusions about the null hypothesis.\n",
    "\n",
    "Data Standardization:\n",
    "\n",
    "The z-score is used in data standardization or normalization. By subtracting the mean and dividing by the standard deviation for each data point, the resulting z-scores transform the data into a distribution with a mean of 0 and a standard deviation of 1. This facilitates the comparison of variables with different scales or units.\n",
    "\n",
    "    In summary, the z-score is an important statistical measure that standardizes data, allows for comparison between different distributions, aids in outlier detection, facilitates hypothesis testing, and supports data standardization. Its use is widespread in various fields, including statistics, data analysis, research, and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8245007c-9380-4f00-9d88-e3ef82f99579",
   "metadata": {},
   "source": [
    "## Q9\n",
    "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.\n",
    "\n",
    "    The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that, under certain conditions, the sampling distribution of the mean of a sufficiently large number of independent and identically distributed random variables will approximate a normal distribution, regardless of the shape of the original population distribution.\n",
    "\n",
    "Key points about the Central Limit Theorem:\n",
    "\n",
    "  Conditions:\n",
    "\n",
    "The Central Limit Theorem holds under the assumption that the random variables being sampled are independent and have a finite mean and standard deviation. The theorem applies to various types of distributions, including those that are not normally distributed.\n",
    "\n",
    "  Sample Size:\n",
    "\n",
    "For the Central Limit Theorem to apply, the sample size should be \"sufficiently large.\" There is no fixed rule for determining the exact sample size required, but as a general guideline, a sample size greater than or equal to 30 is often considered sufficient in practice.\n",
    "\n",
    "  Sampling Distribution of the Mean: \n",
    "\n",
    "The Central Limit Theorem specifically focuses on the sampling distribution of the mean. It states that as the sample size increases, the distribution of sample means approaches a normal distribution, centered around the population mean.\n",
    "\n",
    "Significance of the Central Limit Theorem:\n",
    "\n",
    "  Approximation of Unknown Distributions:\n",
    "\n",
    "The Central Limit Theorem allows us to make inferences about population means, even when the underlying population distribution is unknown or not normally distributed. This is particularly useful in situations where assumptions of normality are required for statistical techniques.\n",
    "\n",
    "  Statistical Inference:\n",
    "  \n",
    "  The Central Limit Theorem forms the basis for many statistical techniques, such as hypothesis testing, confidence intervals, and regression analysis. It provides a theoretical foundation for making inferences about population parameters based on sample means.\n",
    "\n",
    "  Real-World Applications: \n",
    "  \n",
    "  The Central Limit Theorem has wide-ranging applications in various fields. It enables researchers to analyze and interpret data, make predictions, and estimate parameters, even when the original data may not follow a normal distribution.\n",
    "\n",
    "  Reliable Estimation:\n",
    "  \n",
    "  The Central Limit Theorem assures us that, with a sufficiently large sample size, the sampling distribution of the mean will be close to normal. This allows for more reliable estimation of population parameters, as the mean is a key summary statistic that is often of interest.\n",
    "\n",
    "In summary, the Central Limit Theorem is a crucial statistical concept that provides a powerful tool for making inferences about population means, even when the underlying population distribution is unknown or non-normal. It has broad applications in statistics, research, and data analysis, enabling reliable estimation and hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66df367a-34fc-49f5-9bf8-992eb47ab398",
   "metadata": {},
   "source": [
    "## Q10\n",
    "Q10: State the assumptions of the Central Limit Theorem.\n",
    "\n",
    "    Apologies for the repetition in the previous response. Here are the assumptions of the Central Limit Theorem (CLT):\n",
    "\n",
    "Independent and Identically Distributed (IID) Random Variables: \n",
    "\n",
    "The random variables in the sample should be independent of each other, meaning that the value of one variable does not depend on or affect the values of the others. Additionally, the random variables should be identically distributed, meaning they follow the same probability distribution.\n",
    "\n",
    "Finite Mean and Variance:\n",
    "\n",
    "The random variables should have a finite mean (μ) and finite variance (σ^2). These assumptions ensure that the sample mean and sample variance are well-defined and that they have meaningful properties as the sample size increases.\n",
    "\n",
    "Sample Size:\n",
    "\n",
    "The sample size should be sufficiently large. While there is no exact threshold, a larger sample size generally leads to better approximations of the sampling distribution. A common guideline is that the sample size should be greater than or equal to 30. However, this guideline may vary depending on the characteristics of the population distribution.\n",
    "\n",
    "No Substantial Skewness or Outliers:\n",
    "\n",
    "The Central Limit Theorem assumes that the underlying population distribution is not heavily skewed and does not contain extreme outliers. Skewness refers to a lack of symmetry in the distribution, where one tail is longer or heavier than the other. Extreme outliers can also disrupt the convergence to a normal distribution.\n",
    "\n",
    "    It's important to note that the Central Limit Theorem is a theoretical result, and its assumptions may not hold exactly in practice. However, in many real-world scenarios, even if the assumptions are not fully satisfied, the Central Limit Theorem can still provide reasonable approximations and be used effectively for statistical inference and hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c3c8a-3023-47aa-afb0-26fed3d0e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2f1ab1-7226-43cb-a4f3-d69055e48dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
