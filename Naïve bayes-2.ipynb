{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942a0f96-28ea-48fa-8550-a511a3b3b419",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d01d26-9878-4f0a-9a54-8cec84e9910f",
   "metadata": {},
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc0c564-090e-4275-ae32-1faa84bb61b3",
   "metadata": {},
   "source": [
    "### Naïve bayes-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4a84e-2601-42b5-ad49-10dbabe6fe82",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' theorem. Let's denote the following events:\n",
    "\n",
    "A: An employee uses the health insurance plan.\n",
    "B: An employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "\n",
    "P(A) = 0.70 (70% of employees use the health insurance plan)\n",
    "P(B|A) = 0.40 (40% of employees who use the plan are smokers)\n",
    "\n",
    "Bayes' theorem states:\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "We need to find P(B|A), the probability that an employee is a smoker given that they use the health insurance plan. We already know P(B|A) = 0.40. We also need to find P(B), the probability that an employee is a smoker, and P(A), the probability that an employee uses the health insurance plan.\n",
    "\n",
    "Since we don't have information about the overall percentage of smokers in the company, we cannot directly calculate P(B). We can, however, use the law of total probability to calculate P(A).\n",
    "\n",
    "The law of total probability states:\n",
    "\n",
    "P(A) = P(A|B) * P(B) + P(A|¬B) * P(¬B)\n",
    "\n",
    "Here, ¬B represents \"not B,\" i.e., an employee is not a smoker.\n",
    "\n",
    "From the given information, we know P(A) = 0.70.\n",
    "\n",
    "Using these equations, we can solve for P(B):\n",
    "\n",
    "P(A) = P(A|B) * P(B) + P(A|¬B) * P(¬B)\n",
    "\n",
    "0.70 = 0.40 * P(B) + P(A|¬B) * (1 - P(B))\n",
    "\n",
    "We can rearrange this equation to solve for P(B):\n",
    "\n",
    "0.70 - P(A|¬B) = 0.40 * P(B) - P(A|¬B) * P(B)\n",
    "\n",
    "0.70 - P(A|¬B) = P(B) * (0.40 - P(A|¬B))\n",
    "\n",
    "P(B) = (0.70 - P(A|¬B)) / (0.40 - P(A|¬B))\n",
    "\n",
    "Now we can substitute the given values to find P(B|A):\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "P(B|A) = (0.40 * [(0.70 - P(A|¬B)) / (0.40 - P(A|¬B))]) / 0.70\n",
    "\n",
    "Keep in mind that we still don't have the value of P(A|¬B), the probability that an employee uses the health insurance plan given that they are not a smoker. Without that information, we cannot provide an exact numerical value for P(B|A).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "## Q2\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the assumptions they make about the data distribution and the feature representation.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Bernoulli Naive Bayes is suitable for binary feature variables, where each feature can take on only two values (0 or 1).\n",
    "It assumes that each feature is conditionally independent of the others given the class variable.\n",
    "It is commonly used for text classification tasks, where the presence or absence of words (binary features) in a document is considered.\n",
    "The input data for Bernoulli Naive Bayes is usually a binary feature matrix.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Multinomial Naive Bayes is suitable for discrete feature variables, where each feature represents the frequency or count of a particular event.\n",
    "It assumes that the feature vectors have a multinomial distribution.\n",
    "It is commonly used for text classification tasks, where the features represent word counts or term frequencies.\n",
    "The input data for Multinomial Naive Bayes is typically a count-based feature matrix, such as the term frequency-inverse document frequency (TF-IDF) matrix.\n",
    "In summary, Bernoulli Naive Bayes is used when dealing with binary features, while Multinomial Naive Bayes is used when working with discrete feature counts or frequencies. Both algorithms assume independence among features, and they are commonly applied to text classification problems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q3\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes assumes that each feature is conditionally independent of the others given the class variable. When it comes to handling missing values, there are a few common approaches:\n",
    "\n",
    "Ignoring missing values: One simple approach is to ignore the missing values altogether. In this case, the missing values are treated as if they were never observed, and the model is trained using the available data. When making predictions for instances with missing values, those missing values are also ignored, and the model uses the available features.\n",
    "\n",
    "Assigning a special value: Another approach is to treat missing values as a separate category or assign a special value to represent them. This allows the model to learn the patterns associated with missing values and make predictions accordingly.\n",
    "\n",
    "Imputation: Imputation involves filling in the missing values with estimated values. There are various imputation techniques available, such as mean imputation (replacing missing values with the mean of the feature), mode imputation (replacing missing values with the mode of the feature), or using more advanced methods like regression imputation or multiple imputation.\n",
    "\n",
    "The choice of handling missing values in Bernoulli Naive Bayes (or any other algorithm) depends on the nature of the data and the specific problem at hand. It's important to consider the potential impact of missing values and the implications of the chosen approach on the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q4\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification problems. Gaussian Naive Bayes is an extension of Naive Bayes that assumes the continuous features follow a Gaussian (normal) distribution. While it is commonly used for binary and binary-like problems, it can also handle multi-class classification tasks.\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes estimates the class probabilities and class-conditional means and variances for each feature. During training, it learns the parameters of the Gaussian distribution for each class and each feature. Then, during prediction, it calculates the probability of each class given the observed feature values and selects the class with the highest probability.\n",
    "\n",
    "The decision rule in Gaussian Naive Bayes can be based on the maximum likelihood estimation or the posterior probability. The class with the highest probability is assigned as the predicted class label.\n",
    "\n",
    "It's worth noting that Naive Bayes classifiers, including Gaussian Naive Bayes, make the assumption of feature independence given the class, which may not always hold true in real-world scenarios. Nevertheless, Gaussian Naive Bayes can still provide a simple and efficient approach for multi-class classification problems, especially when the feature distribution approximates a Gaussian distribution and the independence assumption is reasonable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q5\n",
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "To complete the assignment, you can follow the steps outlined below:\n",
    "\n",
    "Data Preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository at the provided link (https://archive.ics.uci.edu/ml/datasets/Spambase).\n",
    "Preprocess the dataset as necessary, including handling missing values, scaling, and splitting into features and target variables.\n",
    "Implementation:\n",
    "3. Import the required libraries in Python, including scikit-learn.\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library.\n",
    "Utilize the 10-fold cross-validation technique to evaluate the performance of each classifier.\n",
    "Train and evaluate each classifier on the Spambase dataset, using the default hyperparameters.\n",
    "Results:\n",
    "7. Calculate and report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy: The proportion of correctly classified instances.\n",
    "Precision: The ability to correctly identify positive instances among the predicted positive instances.\n",
    "Recall: The ability to correctly identify positive instances among the actual positive instances.\n",
    "F1 score: The harmonic mean of precision and recall, providing a balanced measure between the two.\n",
    "Discussion:\n",
    "8. Analyze the obtained results and discuss which variant of Naive Bayes performed the best. Consider factors such as accuracy, precision, recall, and F1 score. Explain why you think that particular variant performed better.\n",
    "\n",
    "Identify any limitations or observations you noticed during the implementation and evaluation of Naive Bayes classifiers.\n",
    "Conclusion:\n",
    "10. Summarize the findings from the experiment, highlighting the performance of each Naive Bayes variant and any relevant observations.\n",
    "\n",
    "Provide suggestions for future work, such as exploring alternative feature representations, handling missing values differently, or applying advanced techniques to address the limitations observed.\n",
    "Please note that implementing the entire assignment falls outside the scope of a single text-based response. You will need to write and execute the code in a Python development environment to complete the assignment successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55da901c-3919-4a8b-bfaf-30c593a67545",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
