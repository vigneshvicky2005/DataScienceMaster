{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fbde62-91e5-4c5e-b464-488aefd49afd",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c41254-3344-469f-9dfa-f7df02483041",
   "metadata": {},
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44700705-9dfd-440b-a5eb-501297dceb42",
   "metadata": {},
   "source": [
    "### Na√Øve bayes-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd77a1e9-45d0-45bc-bcbf-844d6d23da8c",
   "metadata": {},
   "source": [
    "## Q1\n",
    "User\n",
    "Q1. What is Bayes' theorem?\n",
    "\n",
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It provides a way to update the probability of an event based on new evidence or information. The theorem mathematically describes how prior beliefs or knowledge, represented as prior probabilities, can be combined with new data to obtain revised or posterior probabilities.\n",
    "\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the posterior probability of event A given event B.\n",
    "P(B|A) is the probability of event B given event A.\n",
    "P(A) and P(B) are the prior probabilities of events A and B, respectively.\n",
    "In simple terms, Bayes' theorem allows us to calculate the probability of event A occurring, given that event B has occurred. It considers the prior probability of event A happening, and then adjusts it based on the likelihood of observing event B if event A were true.\n",
    "\n",
    "Bayes' theorem is widely used in various fields, including statistics, machine learning, artificial intelligence, and data analysis. It is particularly valuable in situations where new evidence or observations need to be incorporated to update and refine existing beliefs or probabilities.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q2\n",
    "Q2. What is the formula for Bayes' theorem?\n",
    "\n",
    "The formula for Bayes' theorem is as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(A|B) is the posterior probability of event A given event B.\n",
    "P(B|A) is the probability of event B given event A.\n",
    "P(A) and P(B) are the prior probabilities of events A and B, respectively.\n",
    "In words, Bayes' theorem states that the probability of event A occurring given that event B has occurred (posterior probability) is equal to the probability of event B occurring given that event A has occurred (likelihood) multiplied by the prior probability of event A, divided by the prior probability of event B.\n",
    "\n",
    "This formula allows for the updating of probabilities based on new information or evidence. By multiplying the prior probability with the likelihood, we obtain the joint probability of both events occurring. Dividing it by the marginal probability of event B normalizes the result to obtain the revised or updated probability of event A given event B.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q3\n",
    "Q3. How is Bayes' theorem used in practice?\n",
    "\n",
    "\n",
    "Bayes' theorem is used in various practical applications across different fields. Here are a few examples of how Bayes' theorem is applied:\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem is used in medical diagnosis to calculate the probability of a patient having a certain condition given the presence of specific symptoms or test results. Prior probabilities, such as the prevalence of the condition in the population, are combined with the likelihoods of observing particular symptoms or test outcomes to update the probability of the diagnosis.\n",
    "\n",
    "Spam Filtering: Bayes' theorem is employed in spam filtering algorithms to classify emails as spam or non-spam. The algorithm uses prior probabilities based on known characteristics of spam and non-spam emails. It then calculates the likelihood of certain words or patterns appearing in the email and combines them to update the probability of the email being spam.\n",
    "\n",
    "Machine Learning: Bayes' theorem is utilized in Bayesian machine learning algorithms, such as Naive Bayes classifiers. These algorithms make predictions or classifications based on training data and incorporate prior probabilities to update the model's estimates. The theorem helps in estimating the probability of a particular class label given observed features.\n",
    "\n",
    "Risk Assessment: Bayes' theorem is applied in risk assessment and decision-making processes. It allows for the incorporation of new evidence or information to update the probability of certain outcomes or events. By considering prior probabilities and conditional probabilities, decision-makers can make more informed choices based on the updated probabilities.\n",
    "\n",
    "Speech and Language Processing: Bayes' theorem is used in various natural language processing tasks, such as language modeling, part-of-speech tagging,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q4\n",
    "Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
    "\n",
    "Bayes' theorem is closely related to conditional probability. Conditional probability refers to the probability of an event A occurring given that another event B has already occurred. It is denoted as P(A|B), read as \"the probability of A given B.\"\n",
    "\n",
    "Bayes' theorem provides a way to calculate the conditional probability, P(A|B), based on prior probabilities and the likelihood of observing B given A. The formula for Bayes' theorem is:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "In this formula, P(A) represents the prior probability of event A, P(B|A) represents the probability of event B given that event A has occurred (likelihood), and P(B) is the prior probability of event B.\n",
    "\n",
    "Essentially, Bayes' theorem allows us to update our prior beliefs or probabilities (P(A)) based on new information (P(B|A)) to obtain the posterior probability (P(A|B)). It provides a formal mathematical framework for incorporating new evidence and adjusting probabilities accordingly.\n",
    "\n",
    "Conditional probability is a key component of Bayes' theorem because it represents the likelihood of observing B given A. By combining conditional probability with prior probabilities, Bayes' theorem enables the calculation of updated probabilities that take into account the relationship between events A and B.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q5\n",
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "\n",
    "When choosing a type of Naive Bayes classifier for a specific problem, it is essential to consider the characteristics of the data and the assumptions made by each variant of the classifier. Here are some factors to consider:\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Data Representation: Multinomial Naive Bayes is commonly used for text classification tasks where the data is represented as word frequency counts or bag-of-words.\n",
    "Assumption: It assumes that the features (words) are generated from a multinomial distribution, which makes it suitable for discrete feature spaces.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Data Representation: Bernoulli Naive Bayes is suitable when the features are binary or represented as presence/absence indicators.\n",
    "Assumption: It assumes that each feature is independent and follows a Bernoulli distribution.\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Data Distribution: Gaussian Naive Bayes is appropriate when the features are continuous and assumed to follow a Gaussian (normal) distribution.\n",
    "Assumption: It assumes that each class follows a multivariate Gaussian distribution, with features being conditionally independent given the class.\n",
    "The choice of the Naive Bayes variant depends on how well the assumptions align with the characteristics of the data. Here are some guidelines:\n",
    "\n",
    "If the features are discrete and represented as word frequencies or bag-of-words, Multinomial Naive Bayes is a common choice.\n",
    "If the features are binary or represented as presence/absence indicators, Bernoulli Naive Bayes is suitable.\n",
    "If the features are continuous and assumed to follow a Gaussian distribution, Gaussian Naive Bayes can be used.\n",
    "It's important to note that the Naive Bayes classifiers assume independence between features given the class, which may not always hold true in real-world scenarios. Despite this simplifying assumption, Naive Bayes classifiers often perform well in practice and are computationally efficient.\n",
    "\n",
    "Experimentation and evaluating the performance of different Naive Bayes variants on your specific problem and dataset can help determine the most suitable choice. Consider factors such as data distribution, feature representation, and the underlying assumptions of each classifier to make an informed decision.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q6\n",
    "Q6. Assignment:\n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "each feature value for each class:\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "A 3 3 4 4 3 3 3\n",
    "B 2 2 1 2 2 2 3\n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "to belong to?\n",
    "\n",
    "To predict the class of the new instance using Naive Bayes, we need to calculate the likelihoods for each class given the feature values and then use Bayes' theorem to determine the posterior probabilities.\n",
    "\n",
    "Let's calculate the likelihoods for each class:\n",
    "\n",
    "For Class A:\n",
    "P(X1=3|A) = 4/13\n",
    "P(X2=4|A) = 3/13\n",
    "\n",
    "For Class B:\n",
    "P(X1=3|B) = 1/7\n",
    "P(X2=4|B) = 3/7\n",
    "\n",
    "Since we assume equal prior probabilities for each class, the prior probabilities P(A) and P(B) are both 0.5.\n",
    "\n",
    "Now, let's calculate the joint probabilities for each class:\n",
    "\n",
    "For Class A:\n",
    "P(A, X1=3, X2=4) = P(X1=3|A) * P(X2=4|A) * P(A) = (4/13) * (3/13) * 0.5\n",
    "\n",
    "For Class B:\n",
    "P(B, X1=3, X2=4) = P(X1=3|B) * P(X2=4|B) * P(B) = (1/7) * (3/7) * 0.5\n",
    "\n",
    "To normalize the probabilities, we divide each joint probability by the sum of the joint probabilities:\n",
    "\n",
    "Normalization factor = P(A, X1=3, X2=4) + P(B, X1=3, X2=4)\n",
    "\n",
    "Finally, we can calculate the posterior probabilities using Bayes' theorem:\n",
    "\n",
    "P(A|X1=3, X2=4) = P(A, X1=3, X2=4) / (Normalization factor)\n",
    "P(B|X1=3, X2=4) = P(B, X1=3, X2=4) / (Normalization factor)\n",
    "\n",
    "Comparing the posterior probabilities, we can determine the predicted class. The class with the higher posterior probability will be the predicted class.\n",
    "\n",
    "Performing the calculations, we find:\n",
    "\n",
    "P(A|X1=3, X2=4) ‚âà 0.594\n",
    "P(B|X1=3, X2=4) ‚âà 0.406\n",
    "\n",
    "Since P(A|X1=3, X2=4) > P(B|X1=3, X2=4), Naive Bayes would predict the new instance to belong to Class A.\n",
    "\n",
    "Therefore, Naive Bayes would classify the new instance with features X1=3 and X2=4 as belonging to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff72c83-b20f-4e56-baf3-455f7c09b531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
