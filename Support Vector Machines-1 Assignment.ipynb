{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d63f3d4a-88f7-485d-adae-f382448f856c",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c883eb-1135-4100-88f7-51a0dab44da3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e7f92-e7c8-472d-9d9c-eb1a282eda0a",
   "metadata": {},
   "source": [
    "### Support Vector Machines-1 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4b6ea-8d27-443c-a9a0-4c5b129c617b",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a set of input vectors X = {x₁, x₂, ..., xn} in a p-dimensional feature space, and their corresponding target labels y = {-1, +1}, the objective of a linear SVM is to find a hyperplane that maximally separates the two classes.\n",
    "\n",
    "The equation for a linear SVM can be written as:\n",
    "\n",
    "w·x + b = 0\n",
    "\n",
    "where:\n",
    "\n",
    "w is the weight vector perpendicular to the hyperplane\n",
    "x is the input vector\n",
    "b is the bias term\n",
    "The decision function of a linear SVM is given by:\n",
    "\n",
    "f(x) = sign(w·x + b)\n",
    "\n",
    "where sign is the sign function that returns -1 if the expression w·x + b is negative, +1 if it is positive, and 0 if it is exactly zero.\n",
    "\n",
    "The goal of training a linear SVM is to find the optimal values of w and b that maximize the margin between the two classes while minimizing the classification errors. This is typically formulated as a constrained optimization problem, with the objective of finding the hyperplane parameters that minimize:\n",
    "\n",
    "(1/2) ||w||² + C ∑(ξᵢ)\n",
    "\n",
    "subject to:\n",
    "yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ for all i = 1, 2, ..., n\n",
    "\n",
    "where:\n",
    "\n",
    "C is the regularization parameter that controls the trade-off between maximizing the margin and allowing misclassifications\n",
    "ξᵢ are slack variables that allow for some misclassification\n",
    "The optimization problem is typically solved using techniques such as quadratic programming or gradient descent methods to find the optimal values of w and b that satisfy the constraints.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q2\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that maximally separates the two classes while minimizing the classification errors. In a linear SVM, the objective function is typically formulated as a constrained optimization problem.\n",
    "\n",
    "The objective function of a linear SVM can be written as follows:\n",
    "\n",
    "minimize: (1/2) ||w||² + C ∑(ξᵢ)\n",
    "\n",
    "subject to: yᵢ(w·xᵢ + b) ≥ 1 - ξᵢ for all i = 1, 2, ..., n\n",
    "\n",
    "where:\n",
    "\n",
    "w is the weight vector perpendicular to the hyperplane\n",
    "b is the bias term\n",
    "xᵢ is the ith input vector\n",
    "yᵢ is the target label of the ith input vector, where yᵢ ∈ {-1, +1}\n",
    "ξᵢ are slack variables that allow for some misclassification\n",
    "C is the regularization parameter that controls the trade-off between maximizing the margin and allowing misclassifications\n",
    "The objective function consists of two terms. The first term, (1/2) ||w||², is the regularization term that encourages a larger margin between the classes. The objective is to minimize the squared Euclidean norm of the weight vector, which corresponds to maximizing the margin between the classes.\n",
    "\n",
    "The second term, C ∑(ξᵢ), represents the classification errors. The objective is to minimize the sum of the slack variables ξᵢ, which penalizes misclassifications. The regularization parameter C controls the balance between maximizing the margin and allowing some misclassification. A smaller value of C allows for a wider margin but may result in more misclassifications, while a larger value of C puts more emphasis on avoiding misclassifications at the cost of a narrower margin.\n",
    "\n",
    "The optimization problem aims to find the values of w and b that minimize the objective function while satisfying the constraints, which require the data points to be correctly classified with a margin of at least 1 or with a small error within the slack variables. The problem can be solved using techniques such as quadratic programming or gradient descent methods.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q3\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "The ker## Q3nel trick is a technique used in Support Vector Machines (SVMs) to handle nonlinearly separable data by implicitly mapping the input data into a higher-dimensional feature space. It allows SVMs to effectively classify data that cannot be linearly separated in the original input space.\n",
    "\n",
    "In the standard formulation of SVMs, the decision boundary is a hyperplane that separates the classes. However, in some cases, the data points may not be linearly separable in the original input space. The kernel trick addresses this issue by transforming the input data into a higher-dimensional feature space where the classes can be linearly separated.\n",
    "\n",
    "The key idea behind the kernel trick is to define a kernel function, also known as a kernel, which computes the inner product between the transformed feature vectors without explicitly calculating the transformation. The kernel function effectively represents the similarity between two data points in the higher-dimensional space.\n",
    "\n",
    "By using a kernel function, the SVM algorithm can operate in the original input space but implicitly perform calculations in the higher-dimensional feature space. This avoids the need to explicitly compute the transformation, which can be computationally expensive or even impossible for very high-dimensional spaces.\n",
    "\n",
    "The kernel trick allows SVMs to handle complex, nonlinear decision boundaries without explicitly dealing with the transformation to the higher-dimensional space. Commonly used kernel functions include the linear kernel, polynomial kernel, Gaussian (RBF) kernel, and sigmoid kernel, among others. These kernel functions define different ways to measure the similarity between data points in the transformed feature space.\n",
    "\n",
    "In summary, the kernel trick enables SVMs to effectively classify nonlinearly separable data by implicitly mapping the data into a higher-dimensional feature space using a kernel function. This approach allows for powerful and flexible classification while avoiding the computational cost of explicitly transforming the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q4\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "In Support Vector Machines (SVMs), support vectors play a crucial role in defining the decision boundary and determining the classification of new data points. Support vectors are the data points from the training set that lie closest to the decision boundary or are involved in determining the margin between the classes.\n",
    "\n",
    "To illustrate the role of support vectors, let's consider a simple example with a binary classification problem. Suppose we have two classes, Class A and Class B, and our goal is to separate them using an SVM.\n",
    "\n",
    "In the following example, we have plotted the data points of two classes, Class A represented by blue circles and Class B represented by red crosses. The dashed line represents the decision boundary of the SVM.\n",
    "\n",
    "css\n",
    "Copy code\n",
    "       B\n",
    "    B  |     A\n",
    "       |\n",
    "-------|-------\n",
    "       |\n",
    "    A  |    B\n",
    "       A\n",
    "In this case, the data points from Class A and Class B are mixed together, and they cannot be perfectly separated by a straight line in the input space.\n",
    "\n",
    "When we train an SVM, it identifies the support vectors, which are the data points lying closest to the decision boundary or involved in determining the margin. These support vectors have a direct influence on the location and orientation of the decision boundary.\n",
    "\n",
    "In the example, the support vectors are denoted by the larger symbols:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "       B\n",
    "    B  |     A\n",
    "       |\n",
    "-------|-------\n",
    "       |\n",
    "    A  |    B\n",
    "       A\n",
    "The support vectors determine the position of the decision boundary by defining a hyperplane that maximizes the margin between the two classes. The margin is the region between the decision boundary and the closest data points from each class. The SVM aims to find the decision boundary that maximizes this margin.\n",
    "\n",
    "During the classification of new data points, the support vectors are used to determine the class label. The decision is made based on which side of the decision boundary the new data point lies. The support vectors that lie on the margin or closer to the opposing class contribute more to the decision process.\n",
    "\n",
    "In this way, the support vectors guide the SVM in making accurate classifications by representing the critical data points that define the decision boundary and contribute to the separation of classes. They are the key elements that influence the model's behavior and generalize its classification ability to new, unseen data.\n",
    "\n",
    "It is worth noting that SVMs are often referred to as \"sparse\" models since the decision boundary is solely determined by the support vectors, and the remaining data points are not used in the final model, as long as they do not lie on or influence the margin. This sparsity property of SVMs allows them to be computationally efficient even with large datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q5\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "Certainly! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM with examples and corresponding graphs.\n",
    "\n",
    "Hyperplane:\n",
    "In SVM, a hyperplane is a decision boundary that separates the data points of different classes. For linearly separable data, the hyperplane is a line in 2D or a plane in higher dimensions. The hyperplane equation is given by w·x + b = 0, where w is the weight vector perpendicular to the hyperplane and b is the bias term.\n",
    "Example:\n",
    "Consider a 2D dataset with two classes, Class A and Class B, represented by blue circles and red crosses, respectively. The dashed line represents the hyperplane.\n",
    "\n",
    "css\n",
    "Copy code\n",
    "    B       B\n",
    "       A\n",
    "    B       B\n",
    "----------- Hyperplane -----------\n",
    "    A       A\n",
    "       B\n",
    "    A       A\n",
    "Marginal Plane:\n",
    "The marginal plane refers to the planes parallel to the hyperplane that touch the closest data points from each class. These planes define the margin, which is the region between them.\n",
    "Example:\n",
    "Using the same dataset, let's visualize the marginal plane with the margin region shaded in gray.\n",
    "\n",
    "css\n",
    "Copy code\n",
    "    B       B\n",
    "       A\n",
    "    B  |    B\n",
    "       |\n",
    "-------|------- Marginal Plane\n",
    "       |\n",
    "    A  |    A\n",
    "       A\n",
    "    A       A\n",
    "Soft Margin:\n",
    "Soft margin SVM allows for some misclassification errors by introducing slack variables (ξ) in the optimization objective. It allows data points to fall inside the margin or even on the wrong side of the margin but with a penalty. This helps to handle partially linearly separable data or data with outliers.\n",
    "Example:\n",
    "Consider a dataset with a single outlier (marked by an 'X') that cannot be linearly separated. The dashed line represents the soft margin hyperplane, and the gray region represents the margin.\n",
    "\n",
    "css\n",
    "Copy code\n",
    "    B       B\n",
    "       A\n",
    "    B  |    B\n",
    "       |\n",
    "-------|------- Soft Margin Hyperplane\n",
    "       |\n",
    "    A  |    A\n",
    "       A\n",
    "    A       A\n",
    "           X\n",
    "Hard Margin:\n",
    "Hard margin SVM aims to find a hyperplane that perfectly separates the classes without allowing any misclassifications. It assumes the data is linearly separable without errors or outliers. Hard margin SVM is more prone to overfitting and may not work well with noisy or overlapping data.\n",
    "Example:\n",
    "Consider a dataset that is perfectly linearly separable without any outliers. The dashed line represents the hard margin hyperplane, and the gray region represents the margin.\n",
    "\n",
    "css\n",
    "Copy code\n",
    "    B       B\n",
    "       A\n",
    "    B       B\n",
    "----------- Hard Margin Hyperplane -----------\n",
    "    A       A\n",
    "       B\n",
    "    A       A\n",
    "Note: The examples and graphs provided here are for illustrative purposes and represent simplified scenarios. In practice, SVMs can handle data in higher dimensions and non-linear decision boundaries using kernel functions and support vectors.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q6\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "\n",
    "Certainly! I can guide you through the implementation of a linear SVM classifier using the Iris dataset in Python. For the bonus task, we'll compare its performance with the scikit-learn implementation.\n",
    "\n",
    "First, make sure you have the necessary libraries installed. We'll be using scikit-learn for the comparison, so install it if you haven't already:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "pip install scikit-learn\n",
    "Now, let's start by loading the Iris dataset and splitting it into training and testing sets:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "Next, we'll implement the linear SVM classifier from scratch. Here's an example implementation using the stochastic gradient descent (SGD) optimization algorithm:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, num_epochs=1000, C=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Training using SGD\n",
    "        for _ in range(self.num_epochs):\n",
    "            for i in range(n_samples):\n",
    "                if y[i] * (np.dot(X[i], self.w) - self.b) >= 1:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w)\n",
    "                else:\n",
    "                    self.w -= self.learning_rate * (2 * self.C * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.learning_rate * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)\n",
    "Now, let's train and evaluate our linear SVM classifier from scratch:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "# Initialize and train the linear SVM classifier\n",
    "svm_scratch = LinearSVM()\n",
    "svm_scratch.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_scratch = svm_scratch.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_scratch = np.mean(y_pred_scratch == y_test)\n",
    "print(\"Accuracy (from scratch):\", accuracy_scratch)\n",
    "Finally, let's compare the performance with the scikit-learn implementation of SVM:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the scikit-learn SVM classifier\n",
    "svm_sklearn = SVC(kernel='linear', C=1.0)\n",
    "svm_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_sklearn = svm_sklearn.predict(X_test)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_sklearn = np.mean(y_pred_sklearn == y_test)\n",
    "print(\"Accuracy (scikit-learn):\", accuracy_sklearn)\n",
    "By comparing the accuracy values from both implementations, you can assess the performance of the SVM classifier from scratch compared to the scikit-learn implementation on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef88e21-d43c-48b3-a9f4-2a806964faf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
