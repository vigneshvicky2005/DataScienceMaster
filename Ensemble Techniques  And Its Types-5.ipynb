{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8a0612-ee8d-4c69-894e-c8c0336d3ba8",
   "metadata": {},
   "source": [
    "# Pwskills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabda06-cf96-4c76-9dab-69178b37b3a0",
   "metadata": {},
   "source": [
    "## Data Science Master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c11280-6879-45f4-817a-ac4cbffe45e7",
   "metadata": {},
   "source": [
    "### Ensemble Techniques And Its Types-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca926d-1fb5-4007-aa85-52228a9fc6af",
   "metadata": {},
   "source": [
    "## Q1\n",
    "Q1. You are work#ng on a mach#ne learn#ng project where you have a dataset conta#n#ng numer#cal and\n",
    "categor#cal features. You have #dent#f#ed that some of the features are h#ghly correlated and there are\n",
    "m#ss#ng values #n some of the columns. You want to bu#ld a p#pel#ne that automates the feature\n",
    "eng#neer#ng process and handles the m#ss#ng valuesD\n",
    "Des#gn a p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Use an automated feature select#on method to #dent#fy the #mportant features #n the datasetC\n",
    "Create a numer#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the numer#cal columns us#ng the mean of the column valuesC\n",
    "Scale the numer#cal columns us#ng standard#sat#onC\n",
    "Create a categor#cal p#pel#ne that #ncludes the follow#ng steps\"\n",
    "Impute the m#ss#ng values #n the categor#cal columns us#ng the most frequent value of the columnC\n",
    "One-hot encode the categor#cal columnsC\n",
    "Comb#ne the numer#cal and categor#cal p#pel#nes us#ng a ColumnTransformerC\n",
    "Use a Random Forest Class#f#er to bu#ld the f#nal modelC\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "Note! Your solut#on should #nclude code sn#ppets for each step of the p#pel#ne, and a br#ef explanat#on of\n",
    "each step. You should also prov#de an #nterpretat#on of the results and suggest poss#ble #mprovements for\n",
    "the p#pel#neD\n",
    "\n",
    "\n",
    "Sure! I can help you design the pipeline for your machine learning project. Below is an outline of the steps involved, along with code snippets and explanations for each step:\n",
    "\n",
    "Step 1: Automated Feature Selection\n",
    "To identify the important features in the dataset, you can use a feature selection method such as Recursive Feature Elimination (RFE) with cross-validation. Here's an example of how you can implement it using scikit-learn:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# X_train: training data (numerical and categorical features)\n",
    "# y_train: target variable\n",
    "\n",
    "# Create a RandomForestClassifier as the estimator\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "# Perform feature selection using RFECV\n",
    "selector = RFECV(estimator, cv=5)\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "Step 2: Numerical Pipeline\n",
    "Now, let's create a pipeline to handle the numerical features. This pipeline will impute missing values with the mean and scale the numerical columns using standardization.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_train_selected: numerical features after feature selection\n",
    "\n",
    "# Create a numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Apply the numerical pipeline to the selected features\n",
    "X_train_numerical = numerical_pipeline.fit_transform(X_train_selected)\n",
    "Step 3: Categorical Pipeline\n",
    "Similarly, let's create a pipeline to handle the categorical features. This pipeline will impute missing values with the most frequent value and perform one-hot encoding.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# X_train: original dataset with categorical features\n",
    "\n",
    "# Create a categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Apply the categorical pipeline to the original dataset\n",
    "X_train_categorical = categorical_pipeline.fit_transform(X_train)\n",
    "Step 4: Combine Pipelines\n",
    "Next, we need to combine the numerical and categorical pipelines using ColumnTransformer.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Combine the numerical and categorical pipelines using ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, X_train_selected.columns),\n",
    "    ('categorical', categorical_pipeline, X_train.columns)\n",
    "])\n",
    "\n",
    "# Apply the preprocessor to the training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "Step 5: Model Training and Evaluation\n",
    "Finally, we can train a Random Forest Classifier on the processed data and evaluate its accuracy on the test dataset.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X_test: test data\n",
    "# y_test: true labels for the test data\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Fit the classifier on the processed training data\n",
    "classifier.fit(X_train_processed, y_train)\n",
    "\n",
    "# Apply the preprocessor to the test data\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test_processed)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "Interpretation and Possible Improvements:\n",
    "The pipeline outlined above helps automate the feature engineering process and handles missing values in both numerical and categorical features. By combining the pipelines using ColumnTransformer, we ensure that all the preprocessing steps are applied correctly to the respective feature types.\n",
    "\n",
    "After training the Random Forest Classifier and evaluating its accuracy on the test dataset, you can interpret the results. The accuracy score indicates the proportion of correctly predicted labels. If the accuracy is high, it suggests that the model is performing well on the given dataset.\n",
    "\n",
    "Possible improvements to the pipeline could include exploring different feature selection methods, trying different imputation strategies, or experimenting with other models to see if they yield better results for your specific dataset. Additionally, you could consider incorporating techniques such as hyperparameter tuning or cross-validation to further optimize the model's performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q2. Bu#ld a p#pel#ne that #ncludes a random forest class#f#er and a log#st#c regress#on class#f#er, and then\n",
    "use a vot#ng class#f#er to comb#ne the#r pred#ct#ons. Tra#n the p#pel#ne on the #r#s dataset and evaluate #ts\n",
    "accuracy.\n",
    "\n",
    "Certainly! Here's an example of a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, combined using a Voting Classifier. We'll train the pipeline on the iris dataset and evaluate its accuracy.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the individual classifiers\n",
    "random_forest = RandomForestClassifier(random_state=42)\n",
    "logistic_regression = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the Voting Classifier\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('rf', random_forest), ('lr', logistic_regression)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "In this pipeline, we create a Random Forest Classifier and a Logistic Regression Classifier. Then, we combine them using a Voting Classifier, which aggregates their predictions using majority voting ('hard' voting strategy). The pipeline is trained on the iris dataset, and the accuracy of the predictions on the test set is computed and printed.\n",
    "\n",
    "You can replace the iris dataset with your own dataset by loading the appropriate data and splitting it into training and test sets using train_test_split(). Additionally, you can modify the classifiers or use more classifiers in the Voting Classifier by adding them to the estimators list.\n",
    "\n",
    "Please note that when using the Voting Classifier, it is important to ensure that the individual classifiers are diverse in their behavior and make different types of errors to achieve better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
